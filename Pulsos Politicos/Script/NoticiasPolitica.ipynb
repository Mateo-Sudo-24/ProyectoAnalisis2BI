{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from couchbase.exceptions import CouchbaseException\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.options import ClusterOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Couchbase\n",
    "endpoint = \"couchbases://cb.pps-dendznwwlsxh.cloud.couchbase.com\"\n",
    "username = \"BDD_PULSOSPOLITICOS\"\n",
    "password = \"Administrador32#\"  # Reemplazar con la contraseña real\n",
    "bucket_name = \"BDD_PULSOSPOLITICOS\"\n",
    "scope_name = \"OpinionPublicaPolitica\"\n",
    "collection_name = \"NoticiasPoliticas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couchbase connection setup\n",
    "auth = PasswordAuthenticator(username, password)\n",
    "options = ClusterOptions(auth)\n",
    "options.apply_profile(\"wan_development\")\n",
    "\n",
    "cluster = Cluster(endpoint, options)\n",
    "cluster.wait_until_ready(timedelta(seconds=5))\n",
    "cb = cluster.bucket(bucket_name)\n",
    "cb_coll = cb.scope(scope_name).collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://cnnespanol.cnn.com/latinoamerica/ecuador/\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "response = requests.get(base_url, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página de noticias políticas en El Telégrafo\n",
    "url = \"https://www.eltelegrafo.com.ec/politica\"\n",
    "\n",
    "# Obtener la respuesta del servidor\n",
    "response = requests.get(url)\n",
    "\n",
    "# Si la respuesta es exitosa, analizamos el contenido con BeautifulSoup\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceso exitoso a la página principal (https://www.teleamazonas.com/)\n",
      "Datos guardados en 'noticias_teleamazonas_principal.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de Teleamazonas\n",
    "url = 'https://www.teleamazonas.com/'\n",
    "\n",
    "# Lista para almacenar las noticias\n",
    "noticias = []\n",
    "\n",
    "# Hacer la solicitud GET para obtener el contenido de la página\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"Acceso exitoso a la página principal ({url})\")\n",
    "    \n",
    "    # Crear el objeto BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar todos los títulos de noticias con la clase 'entry-title'\n",
    "    articulos = soup.find_all('h3', class_='entry-title')\n",
    "    \n",
    "    # Extraer el título y el enlace de cada artículo\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)  # Extraer el texto del título\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None  # Extraer el enlace\n",
    "        \n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace\n",
    "            })\n",
    "else:\n",
    "    print(f\"Error al acceder a la página principal. Código de estado: {response.status_code}\")\n",
    "\n",
    "# Convertir las noticias extraídas en un DataFrame de pandas\n",
    "df_noticias = pd.DataFrame(noticias)\n",
    "\n",
    "# Guardar el DataFrame en un archivo JSON\n",
    "df_noticias.to_json('noticias_teleamazonas_principal.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Datos guardados en 'noticias_teleamazonas_principal.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceso exitoso a la página principal (https://www.eluniverso.com/)\n",
      "Datos guardados en 'noticias_eluniverso_principal.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de El Universo\n",
    "url = 'https://www.eluniverso.com/'\n",
    "\n",
    "# Lista para almacenar las noticias\n",
    "noticias = []\n",
    "\n",
    "# Hacer la solicitud GET para obtener el contenido de la página\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"Acceso exitoso a la página principal ({url})\")\n",
    "    \n",
    "    # Crear el objeto BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar todos los títulos de noticias con la clase 'text-base m-0 font-bold font-primary'\n",
    "    articulos = soup.find_all('h2', class_='text-base m-0 font-bold font-primary')\n",
    "    \n",
    "    # Extraer el título y el enlace de cada artículo\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)  # Extraer el texto del título\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None  # Extraer el enlace\n",
    "        \n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': 'https://www.eluniverso.com' + enlace  # Concatenar la URL base\n",
    "            })\n",
    "else:\n",
    "    print(f\"Error al acceder a la página principal. Código de estado: {response.status_code}\")\n",
    "\n",
    "# Convertir las noticias extraídas en un DataFrame de pandas\n",
    "df_noticias = pd.DataFrame(noticias)\n",
    "\n",
    "# Guardar el DataFrame en un archivo JSON\n",
    "df_noticias.to_json('noticias_eluniverso_principal.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Datos guardados en 'noticias_eluniverso_principal.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista para almacenar las noticias\n",
    "noticias = []\n",
    "\n",
    "# Buscar todos los artículos de noticias en el HTML\n",
    "for item in soup.find_all('div', class_='catItemView groupLeading'):\n",
    "    titulo_element = item.find('h1', class_='story-heading')\n",
    "    if titulo_element:\n",
    "        titulo = titulo_element.get_text(strip=True)\n",
    "        enlace = titulo_element.find('a').get('href')\n",
    "        if enlace and enlace.startswith('/'):\n",
    "            enlace = f\"https://www.eltelegrafo.com.ec{enlace}\"\n",
    "        noticias.append({'titulo': titulo, 'enlace': enlace})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados en 'noticias_ecuador.json'\n"
     ]
    }
   ],
   "source": [
    "# Verificar que la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los artículos de noticias con la clase 'container__headline-text'\n",
    "    noticias = soup.find_all('span', class_='container__headline-text')\n",
    "    \n",
    "    # Listas para almacenar los datos\n",
    "    titulos = []\n",
    "    enlaces = []\n",
    "    \n",
    "    # Recorrer las noticias y extraer los títulos y los enlaces\n",
    "    for noticia in noticias:\n",
    "        # Buscar el enlace en el elemento contenedor\n",
    "        enlace = noticia.find_parent('a')['href']\n",
    "        \n",
    "        # Formar la URL completa en caso de que el enlace sea relativo\n",
    "        if enlace.startswith('/'):\n",
    "            enlace = 'https://cnnespanol.cnn.com' + enlace\n",
    "        \n",
    "        # Agregar el título y el enlace a las listas\n",
    "        titulos.append(noticia.get_text())\n",
    "        enlaces.append(enlace)\n",
    "    \n",
    "    # Crear un DataFrame de Pandas\n",
    "    df = pd.DataFrame({\n",
    "        'titulo': titulos,\n",
    "        'link': enlaces\n",
    "    })\n",
    "    \n",
    "    # Guardar los resultados en un archivo JSON\n",
    "    df.to_json('noticias_ecuador.json', orient='records', force_ascii=False, lines=True)\n",
    "    \n",
    "    print(\"Datos guardados en 'noticias_ecuador.json'\")\n",
    "else:\n",
    "    print(\"Error al acceder a la página:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página principal de Expreso\n",
    "url = 'https://www.expreso.ec'\n",
    "\n",
    "# Realizar la solicitud GET\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Crear una lista para almacenar las noticias\n",
    "noticias = []\n",
    "\n",
    "# Buscar los títulos y enlaces de las noticias dentro de <h2 class=\"c-article__title\">\n",
    "articulos = soup.find_all('h2', class_='c-article__title')\n",
    "\n",
    "# Iterar sobre los artículos encontrados y extraer el título y el enlace\n",
    "for articulo in articulos:\n",
    "    titulo = articulo.get_text(strip=True)\n",
    "    enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "    if enlace:  # Verifica si el enlace no es None\n",
    "        noticias.append({\n",
    "            'titulo': titulo,\n",
    "            'enlace': url + enlace  # Agrega la URL base al enlace relativo\n",
    "        })\n",
    "\n",
    "# Convertir la lista de noticias en un DataFrame de pandas\n",
    "df_noticias = pd.DataFrame(noticias)\n",
    "\n",
    "# Guardar los datos en un archivo JSON\n",
    "df_noticias.to_json('noticias_expreso.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "# Imprimir el DataFrame\n",
    "print(df_noticias)\n",
    "\n",
    "print(\"Datos guardados en 'noticias_expreso.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página principal de Expreso\n",
    "url = 'https://www.expreso.ec'\n",
    "\n",
    "# Realizar la solicitud GET\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Crear una lista para almacenar las noticias\n",
    "noticias = []\n",
    "\n",
    "# Buscar los títulos y enlaces de las noticias dentro de <h2 class=\"c-article__title\">\n",
    "articulos = soup.find_all('h2', class_='c-article__title')\n",
    "\n",
    "# Iterar sobre los artículos encontrados y extraer el título y el enlace\n",
    "for articulo in articulos:\n",
    "    titulo = articulo.get_text(strip=True)\n",
    "    enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "    if enlace:  # Verifica si el enlace no es None\n",
    "        noticias.append({\n",
    "            'titulo': titulo,\n",
    "            'enlace': url + enlace  # Agrega la URL base al enlace relativo\n",
    "        })\n",
    "\n",
    "# Convertir la lista de noticias en un DataFrame de pandas\n",
    "df_noticias = pd.DataFrame(noticias)\n",
    "\n",
    "# Guardar los datos en un archivo JSON\n",
    "df_noticias.to_json('noticias_expreso.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "# Imprimir el DataFrame\n",
    "print(df_noticias)\n",
    "\n",
    "print(\"Datos guardados en 'noticias_expreso.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceso exitoso a la página.\n",
      "Datos guardados en 'noticias_radio_sucre.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de Radio Sucre\n",
    "url = \"https://radiosucre.com.ec\"\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(\"Acceso exitoso a la página.\")\n",
    "    \n",
    "    # Analizar el contenido con BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Encontrar los títulos de las noticias\n",
    "    articulos = soup.find_all('h4', class_='title')\n",
    "    \n",
    "    # Lista para almacenar las noticias\n",
    "    noticias = []\n",
    "    \n",
    "    # Extraer los títulos y enlaces\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href']\n",
    "        \n",
    "        noticias.append({\n",
    "            'titulo': titulo,\n",
    "            'enlace': enlace\n",
    "        })\n",
    "    \n",
    "    # Convertir a DataFrame de Pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "    \n",
    "    # Guardar como archivo JSON\n",
    "    df.to_json('noticias_radio_sucre.json', orient='records', lines=True, force_ascii=False)\n",
    "    \n",
    "    print(\"Datos guardados en 'noticias_radio_sucre.json'\")\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceso exitoso a la página.\n",
      "Datos guardados en 'noticias_notimundo.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de Notimundo\n",
    "url = \"https://notimundo.com.ec\"\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(\"Acceso exitoso a la página.\")\n",
    "    \n",
    "    # Analizar el contenido con BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Encontrar los títulos de las noticias\n",
    "    articulos = soup.find_all('h3', class_='entry-title td-module-title')\n",
    "    \n",
    "    # Lista para almacenar las noticias\n",
    "    noticias = []\n",
    "    \n",
    "    # Extraer los títulos y enlaces\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href']\n",
    "        \n",
    "        noticias.append({\n",
    "            'titulo': titulo,\n",
    "            'enlace': enlace\n",
    "        })\n",
    "    \n",
    "    # Convertir a DataFrame de Pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "    \n",
    "    # Guardar como archivo JSON\n",
    "    df.to_json('noticias_notimundo.json', orient='records', lines=True, force_ascii=False)\n",
    "    \n",
    "    print(\"Datos guardados en 'noticias_notimundo.json'\")\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.vistazo.com/\n",
      "Datos guardados en 'noticias_vistazo.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de Vistazo\n",
    "url = 'https://www.vistazo.com/'\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h2>\n",
    "    articulos = soup.find_all('h2')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find_parent('a')['href'] if articulo.find_parent('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': 'https://www.vistazo.com' + enlace  # Añadir el dominio al enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_vistazo.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_vistazo.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.extra.ec/\n",
      "Datos guardados en 'noticias_extra.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de Extra\n",
    "url = 'https://www.extra.ec/'\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h2> con la clase 'c-article__title'\n",
    "    articulos = soup.find_all('h2', class_='c-article__title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': 'https://www.extra.ec' + enlace  # Añadir el dominio al enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_extra.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_extra.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.hoy.com.ec/\n",
      "Datos guardados en 'noticias_hoy.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de Hoy\n",
    "url = 'https://www.hoy.com.ec/'\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h1> con la clase 'blog-entry-title entry-title'\n",
    "    articulos = soup.find_all('h1', class_='blog-entry-title entry-title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': 'https://www.hoy.com.ec' + enlace  # Añadir el dominio al enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_hoy.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_hoy.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.larepublica.ec/\n",
      "Datos guardados en 'noticias_la_republica.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de La República\n",
    "url = 'https://www.larepublica.ec/'\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h3> con la clase 'mh-custom-posts-xl-title'\n",
    "    articulos = soup.find_all('h3', class_='mh-custom-posts-xl-title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace  # El enlace ya está completo\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_la_republica.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_la_republica.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.planv.com.ec\n",
      "Datos guardados en 'noticias_planv.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de PlanV\n",
    "url = 'https://www.planv.com.ec'\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h3> con la clase 'entry-title td-module-title'\n",
    "    articulos = soup.find_all('h3', class_='entry-title td-module-title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace  # El enlace ya está completo\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_planv.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_planv.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.ecuadortimes.net/\n",
      "Datos guardados en 'noticias_ecuador_times.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de Ecuador Times\n",
    "url = 'https://www.ecuadortimes.net/'\n",
    "\n",
    "# Encabezados para simular un navegador real\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h2> con la clase 'blog-title'\n",
    "    articulos = soup.find_all('h2', class_='blog-title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        enlace = articulo.find('a')  # Buscar el enlace dentro del h2\n",
    "        if enlace:\n",
    "            titulo = enlace.get_text(strip=True)\n",
    "            url_noticia = enlace['href']  # Obtener el atributo href del enlace\n",
    "            \n",
    "            # Añadir la noticia a la lista\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': url_noticia\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_ecuador_times.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_ecuador_times.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.bbc.com/news\n",
      "Datos guardados en 'noticias_bbc.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de BBC News\n",
    "url = 'https://www.bbc.com/news'\n",
    "\n",
    "# Encabezados para evitar restricciones\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h2> con el atributo data-testid=\"card-headline\"\n",
    "    articulos = soup.find_all('h2', {'data-testid': 'card-headline'})\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find_parent('a')['href'] if articulo.find_parent('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            # Asegurar que el enlace sea completo\n",
    "            if enlace.startswith('/'):\n",
    "                enlace = f\"https://www.bbc.com{enlace}\"\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_bbc.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_bbc.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.cnn.com/\n",
      "Datos guardados en 'noticias_cnn.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de CNN\n",
    "url = 'https://www.cnn.com/'\n",
    "\n",
    "# Encabezados para evitar restricciones\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h2> con la clase específica\n",
    "    articulos = soup.find_all('h2', class_='container__title_url-text container_lead-package__title_url-text')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find_parent('a')['href'] if articulo.find_parent('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            # Asegurar que el enlace sea completo\n",
    "            if enlace.startswith('/'):\n",
    "                enlace = f\"https://www.cnn.com{enlace}\"\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_cnn.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_cnn.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.theguardian.com/\n",
      "Datos guardados en 'noticias_the_guardian.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de The Guardian\n",
    "url = 'https://www.theguardian.com/'\n",
    "\n",
    "# Encabezados para evitar restricciones\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <a> con el atributo 'data-link-name'\n",
    "    articulos = soup.find_all('a', attrs={'data-link-name': lambda x: x and 'card' in x})\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get('aria-label', 'Sin título').strip()\n",
    "        enlace = articulo['href'] if articulo.has_attr('href') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            # Asegurar que el enlace sea completo\n",
    "            if enlace.startswith('/'):\n",
    "                enlace = f\"https://www.theguardian.com{enlace}\"\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_the_guardian.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_the_guardian.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.aljazeera.com/\n",
      "Datos guardados en 'noticias_aljazeera.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de Al Jazeera\n",
    "url = 'https://www.aljazeera.com/'\n",
    "\n",
    "# Encabezados para evitar restricciones\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h3> con la clase 'gc__title'\n",
    "    articulos = soup.find_all('h3', class_='gc__title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        enlace_element = articulo.find('a', class_='u-clickable-card__link')\n",
    "        titulo = enlace_element.find('span').get_text(strip=True) if enlace_element else \"Sin título\"\n",
    "        enlace = enlace_element['href'] if enlace_element and enlace_element.has_attr('href') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            # Asegurar que el enlace sea completo\n",
    "            if enlace.startswith('/'):\n",
    "                enlace = f\"https://www.aljazeera.com{enlace}\"\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_aljazeera.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_aljazeera.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio donde están los archivos JSON\n",
    "directorio_json = './json_files/'  # Cambia esta ruta si es necesario\n",
    "directorio_csv = './csv_files/'  # Carpeta para guardar los CSV\n",
    "\n",
    "# Asegurarse de que las carpetas existen\n",
    "os.makedirs(directorio_json, exist_ok=True)\n",
    "os.makedirs(directorio_csv, exist_ok=True)\n",
    "\n",
    "# Procesar cada archivo JSON\n",
    "for archivo in os.listdir(directorio_json):\n",
    "    if archivo.endswith('.json'):\n",
    "        # Cargar JSON como DataFrame\n",
    "        ruta_json = os.path.join(directorio_json, archivo)\n",
    "        df = pd.read_json(ruta_json, lines=True)\n",
    "        \n",
    "        # Guardar como CSV\n",
    "        nombre_csv = archivo.replace('.json', '.csv')\n",
    "        ruta_csv = os.path.join(directorio_csv, nombre_csv)\n",
    "        df.to_csv(ruta_csv, index=False, encoding='utf-8')\n",
    "        print(f\"Convertido: {archivo} -> {nombre_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV content inserted into Couchbase.\n"
     ]
    }
   ],
   "source": [
    "# Subir el archivo CSV a Couchbase \n",
    "try:\n",
    "    with open('noticias_ecuador.csv', 'r', encoding='utf-8') as f:\n",
    "        csv_content = f.read()\n",
    "    \n",
    "    cb_coll.insert('noticias_ecuador_csv', {'content': csv_content})\n",
    "    print(\"CSV content inserted into Couchbase.\")\n",
    "except CouchbaseException as e:\n",
    "    print(f\"Error inserting CSV content into Couchbase: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"General error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los CSV se han combinado en 'noticias_ecuador.csv'\n"
     ]
    }
   ],
   "source": [
    "# Lista para almacenar los DataFrames\n",
    "csv_combinados = []\n",
    "\n",
    "# Leer todos los CSV en el directorio\n",
    "for archivo_csv in os.listdir(directorio_csv):\n",
    "    if archivo_csv.endswith('.csv'):\n",
    "        ruta_csv = os.path.join(directorio_csv, archivo_csv)\n",
    "        df = pd.read_csv(ruta_csv)\n",
    "        csv_combinados.append(df)\n",
    "\n",
    "# Combinar todos los DataFrames en uno solo\n",
    "df_combinado = pd.concat(csv_combinados, ignore_index=True)\n",
    "\n",
    "# Guardar el archivo combinado como CSV\n",
    "df_combinado.to_csv('noticias_ecuador.csv', index=False, encoding='utf-8')\n",
    "print(\"Todos los CSV se han combinado en 'noticias_ecuador.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV content inserted into Couchbase.\n"
     ]
    }
   ],
   "source": [
    "# Subir el archivo CSV a Couchbase \n",
    "try:\n",
    "    with open('noticias_ecuador.csv', 'r', encoding='utf-8') as f:\n",
    "        csv_content = f.read()\n",
    "    \n",
    "    cb_coll.insert('noticias_ecuador_csv', {'content': csv_content})\n",
    "    print(\"CSV content inserted into Couchbase.\")\n",
    "except CouchbaseException as e:\n",
    "    print(f\"Error inserting CSV content into Couchbase: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"General error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
